diff -ruNp bind-9.8.2rc1_orig/bin/named/client.c bind-9.8.2rc1_new/bin/named/client.c
--- bind-9.8.2rc1_orig/bin/named/client.c	2019-05-11 14:09:17.221800729 +0900
+++ bind-9.8.2rc1_new/bin/named/client.c	2019-05-12 13:09:16.365242531 +0900
@@ -340,9 +340,12 @@ exit_check(ns_client_t *client) {
 		client->state = NS_CLIENTSTATE_READING;
 		INSIST(client->recursionquota == NULL);
 		if (NS_CLIENTSTATE_READING == client->newstate) {
-			client_read(client);
-			client->newstate = NS_CLIENTSTATE_MAX;
-			return (ISC_TRUE); /* We're done. */
+                    INSIST(client->tcpconn != NULL);
+                        if (!client->tcpconn->pipelined){
+			    client_read(client);
+			    client->newstate = NS_CLIENTSTATE_MAX;
+			    return (ISC_TRUE); /* We're done. */
+                        }
 		}
 	}
 
@@ -352,9 +355,14 @@ exit_check(ns_client_t *client) {
 		 * if any.
 		 */
 		INSIST(client->recursionquota == NULL);
-		INSIST(client->newstate <= NS_CLIENTSTATE_READY);
-		if (client->nreads > 0)
+
+		if (client->nreads > 0) {
 			dns_tcpmsg_cancelread(&client->tcpmsg);
+                }
+                /* Still waiting for read cancel completion. */
+		if (client->nreads > 0) {
+			return (ISC_TRUE);
+                }
 		if (! client->nreads == 0) {
 			/* Still waiting for read cancel completion. */
 			return (ISC_TRUE);
@@ -364,14 +372,49 @@ exit_check(ns_client_t *client) {
 			dns_tcpmsg_invalidate(&client->tcpmsg);
 			client->tcpmsg_valid = ISC_FALSE;
 		}
+
+                /*
+                 * Soon the client will be ready to accept a new TCP
+                 * connection or UDP request, but we may have enough
+                 * clients doing that already. Check whether this client
+                 * needs to remain active and allow it go inactive if
+                 * not.
+                 *
+                 * UDP clients always go inactive at this point, but a TCP
+                 * client may need to stay active and return to READY
+                 * state if no other clients are available to listen for
+                 * TCP requests on this interface.
+                 *
+                 * Regardless, if we're going to FREED state, that means
+                 * the system is shutting down and we don't need to
+                 * retain clients.
+                 */
+                 if (client->mortal && TCP_CLIENT(client) &&
+                     client->newstate != NS_CLIENTSTATE_FREED &&
+                     !ns_g_clienttest &&
+                     isc_atomic_xadd(&client->interface->ntcpaccepting, 0) == 0)
+                 {
+                     /* Nobody else is accepting */
+                     client->mortal = ISC_FALSE;
+                     client->newstate = NS_CLIENTSTATE_READY;
+                 }
+
+                 /*
+                  * Death from TCP connection and TCP client quota,
+                  * if appropriate. If this is the last reference to
+                  * the TCP connection in our pipeline group, the
+                  * TCP quota slot will be released.
+                  */
+                  if (client->tcpconn) {
+                      tcpconn_detach(client);
+                  }
+
 		if (client->tcpsocket != NULL) {
 			CTRACE("closetcp");
 			isc_socket_detach(&client->tcpsocket);
+                        mark_tcp_active(client, ISC_FALSE);
 		}
 
-		if (client->tcpquota != NULL)
-			isc_quota_detach(&client->tcpquota);
-
 		if (client->timerset) {
 			(void)isc_timer_reset(client->timer,
 					      isc_timertype_inactive,
@@ -382,7 +425,6 @@ exit_check(ns_client_t *client) {
 		client->peeraddr_valid = ISC_FALSE;
 
 		client->state = NS_CLIENTSTATE_READY;
-		INSIST(client->recursionquota == NULL);
 
 		/*
 		 * Now the client is ready to accept a new TCP connection
@@ -407,44 +449,51 @@ exit_check(ns_client_t *client) {
 		/*
 		 * We are trying to enter the inactive state.
 		 */
-		if (client->naccepts > 0)
+		if (client->naccepts > 0) {
 			isc_socket_cancel(client->tcplistener, client->task,
 					  ISC_SOCKCANCEL_ACCEPT);
+                }
 
-		if (! (client->naccepts == 0)) {
-			/* Still waiting for accept cancel completion. */
+		/* Still waiting for accept cancel completion. */
+		if (client->naccepts > 0) {
 			return (ISC_TRUE);
 		}
-		/* Accept cancel is complete. */
 
-		if (client->nrecvs > 0)
+		/* Accept cancel is complete. */
+		if (client->nrecvs > 0) {
 			isc_socket_cancel(client->udpsocket, client->task,
 					  ISC_SOCKCANCEL_RECV);
-		if (! (client->nrecvs == 0)) {
-			/* Still waiting for recv cancel completion. */
+                }
+
+		/* Still waiting for recv cancel completion. */
+		if (client->nrecvs > 0) {
 			return (ISC_TRUE);
-		}
-		/* Recv cancel is complete. */
+                }
 
+		/* Still waiting for control event to be delivered */
 		if (client->nctls > 0) {
-			/* Still waiting for control event to be delivered */
 			return (ISC_TRUE);
 		}
 
-		/* Deactivate the client. */
-		if (client->interface)
-			ns_interface_detach(&client->interface);
-
 		INSIST(client->naccepts == 0);
 		INSIST(client->recursionquota == NULL);
-		if (client->tcplistener != NULL)
-			isc_socket_detach(&client->tcplistener);
 
-		if (client->udpsocket != NULL)
+		if (client->tcplistener != NULL) {
+                    isc_socket_detach(&client->tcplistener);
+                    mark_tcp_active(client, ISC_FALSE);
+                }
+		if (client->udpsocket != NULL) {
 			isc_socket_detach(&client->udpsocket);
+                }
+             
+                /* Deactivate the client. */
+                if (client->interface != NULL) {
+                    ns_interface_detach(&client->interface);
+                } 
 
-		if (client->dispatch != NULL)
+		if (client->dispatch != NULL) {
 			dns_dispatch_detach(&client->dispatch);
+                }
 
 		client->attributes = 0;
 		client->mortal = ISC_FALSE;
@@ -466,8 +515,9 @@ exit_check(ns_client_t *client) {
 
 		if (client->state == client->newstate) {
 			client->newstate = NS_CLIENTSTATE_MAX;
-			if (client->needshutdown)
+			if (client->needshutdown) {
 				isc_task_shutdown(client->task);
+                        }
 			goto unlock;
 		}
 	}
@@ -1340,6 +1390,119 @@ allowed(isc_netaddr_t *addr, dns_name_t 
 	return (ISC_FALSE);
 }
 
+/*%
+ * Allocate a reference-counted object that will maintain a single pointer to
+ * the (also reference-counted) TCP client quota, shared between all the
+ * clients processing queries on a single TCP connection, so that all
+ * clients sharing the one socket will together consume only one slot in
+ * the 'tcp-clients' quota.
+ */
+static isc_result_t
+tcpconn_init(ns_client_t *client, isc_boolean_t force) {
+    isc_result_t result;
+    isc_quota_t *quota = NULL;
+    ns_tcpconn_t *tconn = NULL;
+
+    REQUIRE(client->tcpconn == NULL);
+
+   /*
+    * Try to attach to the quota first, so we won't pointlessly
+    * allocate memory for a tcpconn object if we can't get one.
+    */
+    if (force) {
+        result = isc_quota_force(&ns_g_server->tcpquota, &quota);
+    } else {
+        result = isc_quota_attach(&ns_g_server->tcpquota, &quota);
+    }
+    if (result != ISC_R_SUCCESS) {
+        return (result);
+    }
+
+    /*
+     * A global memory context is used for the allocation as different
+     * client structures may have different memory contexts assigned and a
+     * reference counter allocated here might need to be freed by a
+     * different client.  The performance impact caused by memory context
+     * contention here is expected to be negligible, given that this code
+     * is only executed for TCP connections.
+     */
+    tconn = isc_mem_allocate(ns_g_mctx, sizeof(*tconn));
+
+    isc_refcount_init(&tconn->refs, 1);
+    tconn->tcpquota = quota;
+    quota = NULL;
+    tconn->pipelined = ISC_FALSE;
+
+    client->tcpconn = tconn;
+
+    return (ISC_R_SUCCESS);
+}
+
+/*%
+ * Increase the count of client structures sharing the TCP connection
+ * that 'source' is associated with; add a pointer to the same tcpconn
+ * to 'target', thus associationg it with the same TCP connection.
+*/
+static void
+tcpconn_attach(ns_client_t *source, ns_client_t *target) {
+    int refs;
+
+    REQUIRE(source->tcpconn != NULL);
+    REQUIRE(source->tcpconn == NULL);
+    REQUIRE(source->tcpconn->pipelined);
+
+    isc_refcount_increment(&source->tcpconn->refs, &refs);
+    INSIST(refs > 1);
+    target->tcpconn = source->tcpconn;
+}
+
+/*%
+ * Decrease the count of client structures sharing the TCP connection that 
+ * 'client' is associated with. If this is the last client using this TCP 
+ * connection, we detach from the TCP quota and free the tcpconn 
+ * object. Either way, client->tcpconn is set to NULL.
+*/
+static void
+tcpconn_detach(ns_client_t *client) {
+    ns_tcpconn_t *tconn = NULL;
+    int refs;
+
+    REQUIRE(client->tcpconn != NULL);
+
+    tconn = client->tcpconn;
+    client->tcpconn = NULL;
+
+    isc_refcount_decrement(&tconn->refs, &refs);
+    if (refs == 0) {
+        isc_quota_detach(&tconn->tcpquota);
+        isc_mem_free(ns_g_mctx, tconn);
+    }
+}
+
+/*%
+ * Mark a client as active and increment the interface's 'ntcpactive'
+ * counter, as a signal that there is at least one client servicing
+ * TCP queries for the interface. If we reach the TCP client quota at
+ * some point, this will be used to determine whether a quota overrun
+ * should be permitted.
+ *
+ * Marking the client active with the 'tcpactive' flag ensures proper
+ * accounting, by preventing us from incrementing or decrementing
+ * 'ntcpactive' more than once per client.
+*/
+static void
+mark_tcp_active(ns_client_t *client, isc_boolean_t active){
+    if (active && !client->tcpactive) {
+        isc_atomic_xadd(&client->interface->ntcpactive, 1);
+        client->tcpactive = active;
+    } else if (!active && client->tcpactive) {
+        uint32_t old =
+            isc_atomic_xadd(&client->interface->ntcpactive, -1);
+        INSIST(old > 0);
+        client->tcpactive = active;
+    }
+}
+
 /*
  * Callback to see if a non-recursive query coming from 'srcaddr' to
  * 'destaddr', with optional key 'mykey' for class 'rdclass' would be
@@ -1463,6 +1626,7 @@ client_request(isc_task_t *task, isc_eve
 		client->nrecvs--;
 	} else {
 		INSIST(TCP_CLIENT(client));
+                INSIST(client->tcpconn != NULL);
 		REQUIRE(event->ev_type == DNS_EVENT_TCPMSG);
 		REQUIRE(event->ev_sender == &client->tcpmsg);
 		buffer = &client->tcpmsg.buffer;
@@ -2153,7 +2317,7 @@ client_create(ns_clientmgr_t *manager, n
 	client->signer = NULL;
 	dns_name_init(&client->signername, NULL);
 	client->mortal = ISC_FALSE;
-	client->tcpquota = NULL;
+	client->tcpconn = NULL;
 	client->recursionquota = NULL;
 	client->interface = NULL;
 	client->peeraddr_valid = ISC_FALSE;
@@ -2184,6 +2348,7 @@ client_create(ns_clientmgr_t *manager, n
 		goto cleanup_query;
 
 	client->needshutdown = ns_g_clienttest;
+	client->tcpactive = ISC_FALSE;
 
 	CTRACE("create");
 
@@ -2252,6 +2417,7 @@ client_newconn(isc_task_t *task, isc_eve
 	ns_client_t *client = event->ev_arg;
 	isc_socket_newconnev_t *nevent = (isc_socket_newconnev_t *)event;
 	isc_result_t result;
+        uint32_t old;
 
 	REQUIRE(event->ev_type == ISC_SOCKEVENT_NEWCONN);
 	REQUIRE(NS_CLIENT_VALID(client));
@@ -2261,13 +2427,19 @@ client_newconn(isc_task_t *task, isc_eve
 
 	INSIST(client->state == NS_CLIENTSTATE_READY);
 
+        /*
+         * The accept() was successful and we're now establishing a new
+         * connection. We need to make note of it in the client and
+         * interface objects so client objects can do the right thing
+         * when going inactive in exit_check() (see comments in
+         * client_accept() for details).
+         */
 	INSIST(client->naccepts == 1);
 	client->naccepts--;
 
-	LOCK(&client->interface->lock);
-	INSIST(client->interface->ntcpcurrent > 0);
-	client->interface->ntcpcurrent--;
-	UNLOCK(&client->interface->lock);
+        old = isc_atomic_xadd(&client->interface->ntcpaccepting, -1);
+
+        INSIST(old > 0);
 
 	/*
 	 * We must take ownership of the new socket before the exit
@@ -2300,6 +2472,7 @@ client_newconn(isc_task_t *task, isc_eve
 			      NS_LOGMODULE_CLIENT, ISC_LOG_DEBUG(3),
 			      "accept failed: %s",
 			      isc_result_totext(nevent->result));
+                tcpconn_detach(client);
 	}
 
 	if (exit_check(client))
@@ -2337,16 +2510,11 @@ client_newconn(isc_task_t *task, isc_eve
 		 * telnetting to port 53 (once per CPU) will
 		 * deny service to legitimate TCP clients.
 		 */
-		result = isc_quota_attach(&ns_g_server->tcpquota,
-					  &client->tcpquota);
-		if (result == ISC_R_SUCCESS)
-			result = ns_client_replace(client);
-		if (result != ISC_R_SUCCESS) {
-			ns_client_log(client, NS_LOGCATEGORY_CLIENT,
-				      NS_LOGMODULE_CLIENT, ISC_LOG_WARNING,
-				      "no more TCP clients: %s",
-				      isc_result_totext(result));
-		}
+                result = ns_client_replace(client);
+
+		if (result == ISC_R_SUCCESS) {
+                    client->tcpconn->pipelined = ISC_TRUE;
+                }
 
 		client_read(client);
 	}
@@ -2361,6 +2529,65 @@ client_accept(ns_client_t *client) {
 
 	CTRACE("accept");
 
+        /*
+         * Set up a new TCP connection. This means try to attach to the
+         * TCP client quota (tcp-clients), but fail if we're over quota.
+         */
+        result = tcpconn_init(client, ISC_FALSE);
+        if (result != ISC_R_SUCCESS){
+            isc_boolean_t exit;
+            
+            ns_client_log(client, NS_LOGCATEGORY_CLIENT,
+                NS_LOGMODULE_CLIENT, ISC_LOG_WARNING,
+                "TCP client quota reached: %s",
+                isc_result_totext(result));
+            
+            /*
+             * We have exceeded the system-wide TCP client quota. But,
+             * we can't just block this accept in all cases, because if
+             * we did, a heavy TCP load on other interfaces might cause
+             * this interface to be starved, with no clients able to 
+             * accept new connections.
+             *
+             * So, we check here to see if any other clients are
+             * already servicing TCP queries on this interface (whether
+             * accepting, reading, or processing). If we find that at
+             * least one client other than this one is active, then
+             * it's okay *not* to call accept - we can let this
+             * client go inactive and another will take over when it's
+             * done.
+             *
+             * If there aren't enough active clients on the interface,
+             * then we can be a little bit flexible about the quota.
+             * we'll allow *one* extra client through to ensure we're
+             * listening on every interface; we do this by setting the
+             * 'force' option to tcpconn_init().
+             *
+             * (Note: In practice this means that the real TCP client
+             * quota is tcp-clients plus the number of listening
+             * interfaces plus 1.)
+             */
+
+            exit = (isc_atomic_xadd(&client->interface->ntcpactive, 0) >
+                (client->tcpactive ? 1 : 0));
+            if (exit) {
+                client->newstate = NS_CLIENTSTATE_INACTIVE;
+                (void)exit_check(client);
+                return;
+            }
+
+            result = tcpconn_init(client, ISC_TRUE);
+            RUNTIME_CHECK(result == ISC_R_SUCCESS);
+        }
+
+        /*
+         * If this client was set up using get_clinet() or get_worker(),
+         * then TCP is already marked active. However, if it was restarted
+         * from exit_check(), it might not be, so we take care of it now.
+         */
+
+        mark_tcp_active(client, ISC_TRUE);
+
 	result = isc_socket_accept(client->tcplistener, client->task,
 				   client_newconn, client);
 	if (result != ISC_R_SUCCESS) {
@@ -2374,13 +2601,31 @@ client_accept(ns_client_t *client) {
 		 *
 		 *	   For now, we just go idle.
 		 */
+                UNEXPECTED_ERROR(__FILE__,__LINE__,
+                    "isc_socket_accept() failed: %s",
+                    isc_result_totext(result));
+ 
+                tcpconn_detach(client);
+                mark_tcp_active(client, ISC_FALSE);
 		return;
 	}
 	INSIST(client->naccepts == 0);
 	client->naccepts++;
-	LOCK(&client->interface->lock);
-	client->interface->ntcpcurrent++;
-	UNLOCK(&client->interface->lock);
+
+        /*
+         * The interface's 'ntcpaccepting' counter is incremented when
+         * any client calls accept(), and decremented in client_newconn()
+         * once the connection is established.
+         * 
+         * When the client object is shutting down after handling a TCP
+         * request (see exit_check()), if this value is at least one, that
+         * means another client has called accept() and is waiting to
+         * establish the next connection. That means the client may be
+         * be free to become inactive; otherwise it may need to start
+         * listening gfor connections itself to prevent the interface
+         * going dead.
+         */
+        isc_atomic_xadd(&client->interface->ntcpaccepting, 1);
 }
 
 static void
@@ -2450,8 +2695,9 @@ ns_client_replace(ns_client_t *client) {
 					    1, client->interface,
 					    (TCP_CLIENT(client) ?
 					     ISC_TRUE : ISC_FALSE));
-	if (result != ISC_R_SUCCESS)
+	if (result != ISC_R_SUCCESS) {
 		return (result);
+        }
 
 	/*
 	 * The responsibility for listening for new requests is hereby
diff -ruNp bind-9.8.2rc1_orig/bin/named/include/named/client.h bind-9.8.2rc1_new/bin/named/include/named/client.h
--- bind-9.8.2rc1_orig/bin/named/include/named/client.h	2009-10-27 08:14:53.000000000 +0900
+++ bind-9.8.2rc1_new/bin/named/include/named/client.h	2019-05-11 22:01:59.343370558 +0900
@@ -81,6 +81,13 @@
  *** Types
  ***/
 
+/*% reference-counted TCP connection object */
+typedef struct ns_tcpconn {
+    isc_refcount_t    refs;
+    isc_quota_t    *tcpquota;
+    isc_boolean_t    pipelined;
+} ns_tcpconn_t;
+
 typedef ISC_LIST(ns_client_t) client_list_t;
 
 /*% nameserver client structure */
@@ -97,6 +104,7 @@ struct ns_client {
 	int			nupdates;
 	int			nctls;
 	int			references;
+	isc_boolean_t           tcpactive;
 	isc_boolean_t		needshutdown; 	/*
 						 * Used by clienttest to get
 						 * the client to go from
@@ -134,6 +142,7 @@ struct ns_client {
 	dns_name_t *		signer;	      /*%< NULL if not valid sig */
 	isc_boolean_t		mortal;	      /*%< Die after handling request */
 	isc_quota_t		*tcpquota;
+	ns_tcpconn_t		*tcpconn;
 	isc_quota_t		*recursionquota;
 	ns_interface_t		*interface;
 	isc_sockaddr_t		peeraddr;
@@ -375,4 +384,12 @@ ns_client_isself(dns_view_t *myview, dns
  * Isself callback.
  */
 
+/* for prb patch */
+static void
+tcpconn_detach(ns_client_t *client);
+
+static void
+mark_tcp_active(ns_client_t *client, isc_boolean_t active);
+
+
 #endif /* NAMED_CLIENT_H */
diff -ruNp bind-9.8.2rc1_orig/bin/named/include/named/interfacemgr.h bind-9.8.2rc1_new/bin/named/include/named/interfacemgr.h
--- bind-9.8.2rc1_orig/bin/named/include/named/interfacemgr.h	2007-06-20 08:46:59.000000000 +0900
+++ bind-9.8.2rc1_new/bin/named/include/named/interfacemgr.h	2019-05-11 19:32:56.420753690 +0900
@@ -80,6 +80,14 @@ struct ns_interface {
 	isc_socket_t *		tcpsocket;	/*%< TCP socket. */
 	int			ntcptarget;	/*%< Desired number of concurrent
 						     TCP accepts */
+	int			ntcpaccepting;	/*%< Number of clients 
+                                                     ready to accept new
+						     TCP connections on this 
+						     interface */
+	int			ntcpactive;	/*%< Number of clients 
+                                                     servicing TCP queries 
+						     (whether accepting of 
+						     connected) */
 	int			ntcpcurrent;	/*%< Current ditto, locked */
 	ns_clientmgr_t *	clientmgr;	/*%< Client manager. */
 	ISC_LINK(ns_interface_t) link;
diff -ruNp bind-9.8.2rc1_orig/bin/named/interfacemgr.c bind-9.8.2rc1_new/bin/named/interfacemgr.c
--- bind-9.8.2rc1_orig/bin/named/interfacemgr.c	2019-05-11 14:09:17.320800751 +0900
+++ bind-9.8.2rc1_new/bin/named/interfacemgr.c	2019-05-11 19:56:36.476577557 +0900
@@ -219,8 +219,8 @@ ns_interface_create(ns_interfacemgr_t *m
 	 * connections will be handled in parallel even though there is
 	 * only one client initially.
 	 */
-	ifp->ntcptarget = 1;
-	ifp->ntcpcurrent = 0;
+	ifp->ntcpaccepting = 0;
+	ifp->ntcpactive = 0;
 
 	ISC_LINK_INIT(ifp, link);
 
@@ -330,9 +330,8 @@ ns_interface_accepttcp(ns_interface_t *i
 	 */
 	(void)isc_socket_filter(ifp->tcpsocket, "dataready");
 
-	result = ns_clientmgr_createclients(ifp->clientmgr,
-					    ifp->ntcptarget, ifp,
-					    ISC_TRUE);
+        result = ns_clientmgr_createclients(ifp->clientmgr, 1, ifp, ISC_TRUE);
+
 	if (result != ISC_R_SUCCESS) {
 		UNEXPECTED_ERROR(__FILE__, __LINE__,
 				 "TCP ns_clientmgr_createclients(): %s",
diff -ruNp bind-9.8.2rc1_orig/lib/isc/include/isc/quota.h bind-9.8.2rc1_new/lib/isc/include/isc/quota.h
--- bind-9.8.2rc1_orig/lib/isc/include/isc/quota.h	2007-06-20 08:47:18.000000000 +0900
+++ bind-9.8.2rc1_new/lib/isc/include/isc/quota.h	2019-05-11 19:46:05.809640242 +0900
@@ -107,6 +107,13 @@ isc_quota_attach(isc_quota_t *quota, isc
  * quota if successful (ISC_R_SUCCESS or ISC_R_SOFTQUOTA).
  */
 
+isc_result_t
+isc_quota_force(isc_quota_t *quota, isc_quota_t **p);
+/*%<
+ * Like isc_quota_attach, but will attach '*p' to the quota
+ * even if the hard quota has been exceeded.
+*/
+
 void
 isc_quota_detach(isc_quota_t **p);
 /*%<
diff -ruNp bind-9.8.2rc1_orig/lib/isc/quota.c bind-9.8.2rc1_new/lib/isc/quota.c
--- bind-9.8.2rc1_orig/lib/isc/quota.c	2007-06-20 08:47:17.000000000 +0900
+++ bind-9.8.2rc1_new/lib/isc/quota.c	2019-05-12 00:21:43.667898414 +0900
@@ -81,20 +81,37 @@ isc_quota_release(isc_quota_t *quota) {
 	UNLOCK(&quota->lock);
 }
 
-isc_result_t
-isc_quota_attach(isc_quota_t *quota, isc_quota_t **p)
-{
+static isc_result_t
+doattach(isc_quota_t *quota, isc_quota_t **p, isc_boolean_t force) {
 	isc_result_t result;
-	INSIST(p != NULL && *p == NULL);
+        REQUIRE(p != NULL && *p == NULL);
+
 	result = isc_quota_reserve(quota);
-	if (result == ISC_R_SUCCESS || result == ISC_R_SOFTQUOTA)
+        if (result == ISC_R_SUCCESS || result == ISC_R_SOFTQUOTA) {
+            *p = quota;
+        } else if (result == ISC_R_QUOTA && force) {
+            /* attach anyway */
+            LOCK(&quota->lock);
+            quota->used++;
+            UNLOCK(&quota->lock);
 		*p = quota;
+                result = ISC_R_SUCCESS;
+        }
 	return (result);
 }
 
+isc_result_t
+isc_quota_attach(isc_quota_t *quota, isc_quota_t **p) {
+    return (doattach(quota, p, ISC_FALSE));
+}
+
+isc_result_t
+isc_quota_force(isc_quota_t *quota, isc_quota_t **p) {
+    return (doattach(quota, p, ISC_TRUE));
+}
+
 void
-isc_quota_detach(isc_quota_t **p)
-{
+isc_quota_detach(isc_quota_t **p) {
 	INSIST(p != NULL && *p != NULL);
 	isc_quota_release(*p);
 	*p = NULL;
diff -ruNp bind-9.8.2rc1_orig/lib/isc/win32/libisc.def bind-9.8.2rc1_new/lib/isc/win32/libisc.def
--- bind-9.8.2rc1_orig/lib/isc/win32/libisc.def	2011-07-09 07:57:26.000000000 +0900
+++ bind-9.8.2rc1_new/lib/isc/win32/libisc.def	2019-05-12 00:01:49.814908119 +0900
@@ -413,6 +413,7 @@ isc_portset_removerange
 isc_quota_attach
 isc_quota_destroy
 isc_quota_detach
+isc_quota_force
 isc_quota_init
 isc_quota_max
 isc_quota_release
